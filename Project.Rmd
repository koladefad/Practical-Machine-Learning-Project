---
title: "Practical Machine Learning_Project Assignment"
author: "Mokolade Fadeyibi"
date: "10/27/2020"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(fig.align = 'center')
```


## Overview
This report is the final course project for the Practical Machine Learning Module. The machine learning code in this report is further applied to 20 test cases in the test dataset for the purpose of prediction.

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset).

## Data Loading and Processing

The training data for this project are available here:

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

The test data are available here:

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

First we set the working directory:

```{r wd}
setwd("~/Documents/R_files/Course 8")
```

The R libraries required for the analysis were then loaded into the R environment:

```{r libraries}
library(knitr)
library(caret)
library(rpart)
library(rpart.plot)
library(rattle)
library(corrplot)
set.seed(121212)
```

Source data was downloaded and loaded:

```{r data download}
trainingdata <- read.csv("pml-training.csv", na.strings = c("NA", ""))
testdata <- read.csv("pml-testing.csv", na.strings = c("NA", ""))
```

For the purpose of model building and cross validation, the training data is partitioned into two: Train set, which contains 70% of the data; and Test set, with 30% of the data.

```{r partition}
inTrain  <- createDataPartition(trainingdata$classe, p=0.7, list=FALSE)
TrainSet <- trainingdata[inTrain, ]
TestSet  <- trainingdata[-inTrain, ]
dim(TrainSet)
dim(TestSet)
```

There are quite a number of missing values in the data. For example, examining the 12th column for NAs:

```{r NA inspection}
sum(is.na(TrainSet[,12]))
```

These will be removed and cleaned as below:

```{r NA removal}
NZV <- nearZeroVar(TrainSet)
TrainSet <- TrainSet[, -NZV]
TestSet  <- TestSet[, -NZV]

NAvalues    <- sapply(TrainSet, function(x) mean(is.na(x))) > 0.95
TrainSet <- TrainSet[, NAvalues==FALSE]
TestSet  <- TestSet[, NAvalues==FALSE]

TrainSet <- TrainSet[, -(1:5)]
TestSet  <- TestSet[, -(1:5)]
dim(TrainSet)
```

After cleaning, we're left with 54 variables in the Train set.

```{r dim TestSet}
dim(TestSet)
```

Similarly, the Test set has 54 variables after cleaning.

## Corelation Analysis

A corelation analysis was performed on the data:

```{r Corr Analysis}
corMatrix <- cor(TrainSet[, -54])
corrplot(corMatrix, order = "hclust" , type = "lower",tl.cex = 0.6)
```

The plot shows the level of relationship between the variables. Variables with high positive correlation are indicated by a dark blue colour while those with high negative correlation are denoted by a dark red colour.

## Prediction Model Building
Our original Training data was earlier partitioned into a Train set and a Test set. The Train set will be applied to our prediction model algorithm, which will then be cross validated with the Test set.

Two prediction models will be utilized to build the algorithm for the Train set. These are i) Decision Tree and ii) Random Forest. The prediction model with the higher accuracy will then be used to predict the Test data.

### a) Decision Tree

```{r Decision Tree Training, message=FALSE, warning=FALSE}
set.seed(121212)
DecTreeFit <- rpart(classe ~ ., data=TrainSet, method="class")
fancyRpartPlot(DecTreeFit)
```

### Cross Validation (Using Test Set)

```{r Decision Tree Testing}
DecTreePrediction <- predict(DecTreeFit, newdata=TestSet, type="class")
DecTreeConfMat <- confusionMatrix(DecTreePrediction, TestSet$classe)
DecTreeConfMat
```

This model has an accuracy of 72.3%.

### b) Random Forest

```{r Random Forest Training, cache=TRUE}
set.seed(121212)
ctrlRF <- trainControl(method="cv", number=3, verboseIter=FALSE)
RandForestFit <- train(classe ~ ., data=TrainSet, method="rf", trControl=ctrlRF)
RandForestFit$finalModel
```

### Cross Validation (Using Test Set)

```{r Random Forest Testing}
RandForestPrediction <- predict(RandForestFit, newdata=TestSet)
RandForestConfMat <- confusionMatrix(RandForestPrediction, TestSet$classe)
RandForestConfMat
```

This model has an accuracy rate of 99.7%.

Therefore, the Random Forest model will be applied to our Test data for prediction.

## Out of Sample Error
From the above results, the Random Forest model has the lower out-of-sample error of 0.3%. This validates our selection of this model for use in predicting our Test data.

## Project Prediction Quiz
The Random Forest model will now be applied to the Test data:

```{r Quiz Prediction}
predictTESTdata <- predict(RandForestFit, newdata=testdata)
predictTESTdata
```


